# SVM-Chinese-Classification
利用支持向量机实现搜狗语料库中文文本分类

## 基本流程
#### 1、准备好数据食材、去停用词并利用结巴(jieba)进行分词处理

进行分词，然后使用停止词表Chinesestopword.txt去除停用词.

```
# 参照代码中的cutWords.py文件
```
#### 2、利用卡方检验*特征选择

卡方检验：在构建每个类别的词向量后，对每一类的每一个单词进行其卡方统计值的计算。

1. 首先对卡方 检验所需的 a、b、c、d 进行计算。
a 为在这个分类下包含这个词的文档数量;
b 为不在该分类下包含这个词的文档数量;
c 为在这个分类下不包含这个词的文档数量; 
d 为不在该分类下，且不包含这个词的文档数量。
2. 然后得到该类中该词的卡方统计值
公式为 float(pow((a*d - b*c), 2)) /float((a+c) * (a+b) * (b+d) * (c+d))。
3. 对每一类别的所有词按卡方值进行排序，取前 k 个作为该类的特征值，这里我们取 k 为 1000
```
# featureSelection.py
```
#### 3、利用TF-IDF算法 进行特征权重计算

TF-IDF算法：
- 全称叫 Term Frequency-Inverse Document Frequency 词频-逆文档频率算法
- 主要用于关键词抽取
- 优点：每个词的权重与特征项在文档中出现的频率成正比，与在整个语料中出现该特征项的文档数成反比。
- 训练文本的特征向量表示数据在 train.svm文件中，测试文本的特征向量表示数据在test.svm 中。
```
# featureWeight.py
```
#### 4、对数据进行归一化处理

- 数据归一化是将数据按比例缩放，使其落入特定的范围。通常情况下，数据归一化将数据缩放到 [0, 1] 或 [-1, 1] 的范围内。

```
# normalization
```

#### 5、划分数据集、训练模型，通过模型进行预测，得到评分

- 将数据集进行划分，80%做训练数据，20%做测试数据
- 训练模型并保存下来
- 用测试集进行预测，得到评分

```
SVMmodel.py 
```

预测结果 Accuracy: 0.9302083333333333

注：本文参考大佬的复现而成，原文链接：https://github.com/alicelmx/SVM-Chinese-Classification